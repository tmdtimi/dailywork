## numpy相关计算


```python
import numpy as np

a=np.array([1,2,3,4])
print(a)
```

    [1 2 3 4]



```python
import time

a=np.random.rand(1000000)
b=np.random.rand(1000000)

tic=time.time()
c=np.dot(a,b)
toc=time.time()
print(c)

print("vectorized version:" + str(1000*(toc-tic))+"ms" )

c=0
tic=time.time()
for i in range(1000000):
    c+=a[i]*b[i]
toc=time.time()
print(c)

print("for loop:" + str(1000*(toc-tic))+"ms" )

```

    250193.50027618813
    vectorized version:1.2943744659423828ms
    250193.50027619625
    for loop:668.8070297241211ms



```python

a=np.random.rand(4)
b=np.random.rand(4)
print(a)
print(b)
c=np.dot(a,b)
print(c)

print(np.exp(a))
print(np.abs(a))

```

    [0.02723638 0.25133569 0.18063676 0.50343637]
    [0.22789543 0.34319234 0.65347189 0.24133525]
    0.33200151883408435
    [1.02761068 1.28574163 1.19797995 1.65439663]
    [0.02723638 0.25133569 0.18063676 0.50343637]



```python
import numpy as np
a=np.array([[1,2,3,4],
          [5,6,7,8],
          [9,10,11,12],
          [13,14,15,16]])
print(a)
```

    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]
     [13 14 15 16]]



```python
cal=a.sum(axis=0)
print(cal)
```

    [28 32 36 40]



```python
percentage=100*a/cal.reshape(1,4)
print(percentage)
```

    [[ 3.57142857  6.25        8.33333333 10.        ]
     [17.85714286 18.75       19.44444444 20.        ]
     [32.14285714 31.25       30.55555556 30.        ]
     [46.42857143 43.75       41.66666667 40.        ]]


### 广播机制 运算时自动补全矩阵


```python
a=np.random.randn(5)
print(a)
```

    [ 2.12151054 -0.61650687  0.96525454  2.11307447  0.30298905]



```python
print(a.shape)
```

    (5,)



```python
print(a.T)
```

    [ 2.12151054 -0.61650687  0.96525454  2.11307447  0.30298905]



```python
print(np.dot(a,a.T))
```

    10.36949013213074



```python
a=np.random.randn(5,1)
print(a)
print(a.shape)
```

    [[-0.30576032]
     [-0.07414381]
     [-1.63965348]
     [ 0.1851507 ]
     [ 0.24203113]]
    (5, 1)



```python
print(a.T)
print(a.T.shape)
assert(a.T.shape==(1,5))
```

    [[-0.30576032 -0.07414381 -1.63965348  0.1851507   0.24203113]]
    (1, 5)



```python
print(np.dot(a,a.T))
```

    [[ 0.03559646  0.11265814 -0.10934488 -0.09177306  0.10379608]
     [ 0.11265814  0.35654833 -0.34606227 -0.29044977  0.32850105]
     [-0.10934488 -0.34606227  0.33588461  0.28190767 -0.31883986]
     [-0.09177306 -0.29044977  0.28190767  0.23660487 -0.26760202]
     [ 0.10379608  0.32850105 -0.31883986 -0.26760202  0.30266006]]


### 1.分别用Math.exp() 和 np.exp()实现 sigmod函数


```python
import math
def my_sigmod (x):
    s=1/(1+math.exp(-x))
    return s

print(my_sigmod(3))

x=[1,2,3]
my_sigmod(x) 

#   Math.exp(x) 参数为一个实数 ， 而深度学习中主要使用 矩阵和向量
#   所以numpy更加实用
```

    0.9525741268224334



    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-85-83b4812672f9> in <module>
          7 
          8 x=[1,2,3]
    ----> 9 my_sigmod(x)
         10 
         11 #   Math.exp(x) 参数为一个实数 ， 而深度学习中主要使用 矩阵和向量


    <ipython-input-85-83b4812672f9> in my_sigmod(x)
          1 import math
          2 def my_sigmod (x):
    ----> 3     s=1/(1+math.exp(-x))
          4     return s
          5 


    TypeError: bad operand type for unary -: 'list'


![image.png](attachment:image.png)


```python
x=np.array([1,2,3])
print(np.exp(x))
```

    [ 2.71828183  7.3890561  20.08553692]


![image.png](attachment:image.png)


```python
print(x+3)
```

    [4 5 6]


![image.png](attachment:image.png)


```python
def sigmoid(x):
    s=1/(1+np.exp(x))
    return s
x=np.random.randn(1,3)
print(x)
sigmoid(x)
```

    [[-1.19139301  0.5361798  -0.49656141]]





    array([[0.76699011, 0.36907671, 0.62165091]])



### 2.对sigmoid函数进行求导可得到一部分的梯度公式

![image.png](attachment:image.png)


```python
def sigmoid_derivative(x):
    s=sigmoid(x)
    return s*(1-s)

x=np.array([1,2,3])
print("sigmoid_derivative(x)="+str(sigmoid_derivative(x)))
```

    sigmoid_derivative(x)=[0.19661193 0.10499359 0.04517666]


### 3.重塑数组

    np.shape()  np.reshape()
   
   ![image.png](attachment:image.png)

![image.png](attachment:image.png)

v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) 

v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c


```python
def image2rec(image):
    """
    Argument:
    image -- a numpy array of shape (length, height, depth)
    
    Returns:
    v -- a vector of shape (length*height*depth, 1)
    """
    v=image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)
    return v
```


```python
image = np.array([[[ 0.67826139,  0.29380381],
        [ 0.90714982,  0.52835647],
        [ 0.4215251 ,  0.45017551]],

       [[ 0.92814219,  0.96677647],
        [ 0.85304703,  0.52351845],
        [ 0.19981397,  0.27417313]],

       [[ 0.60659855,  0.00533165],
        [ 0.10820313,  0.49978937],
        [ 0.34144279,  0.94630077]]])
```


```python
image2rec(image)
```




    array([[0.67826139],
           [0.29380381],
           [0.90714982],
           [0.52835647],
           [0.4215251 ],
           [0.45017551],
           [0.92814219],
           [0.96677647],
           [0.85304703],
           [0.52351845],
           [0.19981397],
           [0.27417313],
           [0.60659855],
           [0.00533165],
           [0.10820313],
           [0.49978937],
           [0.34144279],
           [0.94630077]])



### 4.行标准化

![image.png](attachment:image.png)


```python
def normalizeRows(x):
    x_norm=np.linalg.norm(x,axis=1,keepdims=True)
    x=x/x_norm
    print(x_norm)
    return x
x = np.array([
    [0, 3, 4],
    [1, 6, 4]])
print("normalizeRows(x) = " + str(normalizeRows(x)))
```

    [[5.        ]
     [7.28010989]]
    normalizeRows(x) = [[0.         0.6        0.8       ]
     [0.13736056 0.82416338 0.54944226]]


### 5.广播和softmax函数

softmax:
![image.png](attachment:image.png)


```python
def softmax(x):
    x_exp=np.exp(x)
    print(x_exp.shape)
    x_sum=np.sum(x_exp,axis=1,keepdims=True)
    print(x_sum.shape)
    s=x_exp/x_sum
    return s
```


```python
x = np.array([
    [9, 2, 5, 0, 0],
    [7, 5, 0, 0 ,0]])
print("softmax(x) = " + str(softmax(x)))
```

    (2, 5)
    (2, 1)
    softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04
      1.21052389e-04]
     [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04
      8.01252314e-04]]


## 向量化

在深度学习中，通常需要处理非常大的数据集。 因此，非计算最佳函数可能会成为算法中的巨大瓶颈，并可能使模型运行一段时间。 为了确保代码的高效计算，我们将使用向量化。 例如，尝试区分点/外部/元素乘积之间的区别。


```python
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]
```


```python
### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic=time.process_time()
dot=0
for i in range(len(x1)):
    dot+=x1[i]*x2[i]
toc=time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

    dot = 278
     ----- Computation time = 0.13861999999775776ms



```python

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    dot = 278
     ----- Computation time = 0.7006390000015017ms



```python
### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
     [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
     [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
     [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
     [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
     [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
     ----- Computation time = 0.8991069999986223ms



```python
### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
     [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
     [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]
     [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
     [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
     [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
     [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]
     ----- Computation time = 0.16883500000020035ms



```python
### CLASSIC ELEMENTWISE IMPLEMENTATION ###
tic = time.process_time()
mul = np.zeros(len(x1))
for i in range(len(x1)):
    mul[i] = x1[i]*x2[i]
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]
     ----- Computation time = 0.21314600000010842ms



```python
### VECTORIZED ELEMENTWISE MULTIPLICATION ###
tic = time.process_time()
mul = np.multiply(x1,x2)
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]
     ----- Computation time = 0.1306310000011024ms



```python
### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###
W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array
tic = time.process_time()
gdot = np.zeros(W.shape[0])
for i in range(W.shape[0]):
    for j in range(len(x1)):
        gdot[i] += W[i,j]*x1[j]
toc = time.process_time()
print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

    gdot = [27.7357436  12.89318618 27.01214063]
     ----- Computation time = 0.25116700000182846ms



```python
### VECTORIZED GENERAL DOT PRODUCT ###
tic = time.process_time()
dot = np.dot(W,x1)
toc = time.process_time()
print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

```

    gdot = [27.7357436  12.89318618 27.01214063]
     ----- Computation time = 0.7321380000000488ms


## 实现L1 L2损失函数

![image.png](attachment:image.png)


```python
# GRADED FUNCTION: L1

def L1(yhat, y):
    """
    Arguments:
    yhat -- vector of size m (predicted labels)
    y -- vector of size m (true labels)
    
    Returns:
    loss -- the value of the L1 loss function defined above
    """
    
    ### START CODE HERE ### (≈ 1 line of code)
    loss = np.sum(np.abs(y - yhat))
    ### END CODE HERE ###
    
    return loss
```


```python
yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L1 = " + str(L1(yhat,y)))

```

    L1 = 1.1


![image.png](attachment:image.png)


```python
def L2(yhat,y):
    loss=np.dot((y-yhat),(y-yhat).T)
    return loss
```


```python
yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L1 = " + str(L2(yhat,y)))
```

    L1 = 0.43

