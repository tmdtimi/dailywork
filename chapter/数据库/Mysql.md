# mysql


![](../pics/1.png)


![](../pics/2.png)



## **一条查询语句的执行过程** ##

系统和Mysql进行交互，驱动会帮我们建立连接。

一次sql查询就会建立一个连接，java 系统在通过 MySQL 驱动和 MySQL 数据库连接的时候是基于 TCP/IP 协议的，所以如果每个请求都是新建连接和销毁连接，那这样势必会造成不必要的浪费和性能的下降。所以采用数据库连接池的方式，如druid、C3P0等。

系统在访问Mysql数据库的时候，建立的连接不是每次请求都去创建的，而是从数据库连接池中获取，这样就解决了反复的创建连接和销毁连接带来的性能损耗问题。

业务系统是并发的，所以Mysql架构体系中也提供了一个池子，作为数据库连接池，双方都是通过数据库连接池来管理连接的。


![](../pics/s1.png)

Mysql可以大体分为Server层和存储引擎层两部分。

Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务 功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在 这一层实现，比如存储过程、触发器、视图等。

而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、 Memory等多个存储引擎。
 
create table建表的时候，如果不指定引擎类型，默认使用的就是InnoDB，也可以指定引擎。

不同的存储引擎是共用一个Server层的，也就是连接器到执行器的部分。

**连接器**

连接器负责和客户端建立连接，获取权限，维持和管理连接。

mysql -h$ip -P$root -u$user -p$password

在完成TCP连接后，验证账号密码。 用户名密码通过后会到权限表中查出你拥有的权限。

连接完成后，若无后续操作，则此连接处于空闲状态。wait_timeout值默认8小时，客户端8小时没动静会自动断开连接。

长连接：连接成功后，客户端持续有请求，则一直使用同一个连接。短链接是指每次执行完很少的几次查询就断开连接，下次查询重新建立。

连接过程比较复杂，一般采用长连接

但是临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候 才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现 象看就是MySQL异常重启了。

解决此问题可以采取两种方案：

1：定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开 连接，之后要查询再重连。

2：可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。

**查询缓存**

Mysql在执行查询请求时，会先到查询缓存中查看是否之前执行过这条语句。如果是，直接返回，如果不是，就会继续后面的执行过程。

执行完成后，执行结果会被存入查询缓存。

查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。

对于更新压力大的数据库 来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。 比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。

可以将参数query_cache_type设置成 DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语 句，可以用SQL_CACHE显式指定

MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有 这个功能了。

**分析器**

如果没有命中查询缓存，就直接开始执行语句了。

对SQL语句进行解析，首先识别，字符串中的关键字。然后进行句法分析，根据语法规则判断是否有错误。

**优化器**

优化器是在表中有多个索引的时候，选择使用哪一个;或者在一个语句有多表关联的时候，决定各个表的连接顺序。 

优化器阶段结束后，语句的执行方案就确定了，然后进行执行器阶段。

**执行器**

开始执行语句。首先判断你对表有没有执行的权限，如果没有则返回错误。

如果有，则打开表继续执行。打开表的时候，执行器会根据表的引擎定义，引用这个引擎提供的接口。 找到这一行。

你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了 多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。

在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 引 rows_examined r 并不是完全相同的。




## 一条更新语句的执行过程 ##

**redo log  bin log**

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高。

Mysql提供一种Wal技术，write-ahead logging :先写日志，再写磁盘。

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里 面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。

InnoDB的redo log是固定大小的，如可以配置未四个文件，每个1GB

![](../pics/s2.png)

write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。 checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件.

write pos和checkpoint之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果write pos 追上checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint推进一下。

有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为crash-safe.

Mysql大体分为两层，Server层主要负责功能层面的事。引擎层负责存储的相关事宜。redo log是InnoDB特有的日志，而Server层也有自己的日志。

早期Mysql并没有InnoDB引擎。自带的是MyISAM，它并没有crash-safe的能力，binlog日志只能用于归档。

InnoDB是以一个插件的形式引入Mysql的。仅靠binlog无法实现crash-safe，所以InnoDB引入了redo log。

redo log 和 bin log 差异：

1. redolog是InnoDB特有的，binlog是Server层实现的。所有引擎都可以使用
2. redolog是物理日志，记录的“在某个数据页做了什么修改”，bin log是逻辑日志，记录的是这个语句的原始逻辑，“给ID=2这一样的c+1”
3. redolog是循环写的，空间固定会用完。binlog是可以追加写入的。写到一定大小后会切换到下一个，并不会覆盖以前的日志。


![](../pics/s3.png)

update内部流程

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 


2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行 数据，再调用引擎接口写入这行新数据。


3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。 


4. 执行器生成这个操作的binlog，并把binlog写入磁盘。 


5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。

**两阶段提交**

两阶段提交-->为了让两份日志之间的逻辑一致。  如何让数据库恢复到半个月内任意一秒的状态？

binlog会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的DBA承 诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有binlog，同时系统会定期 做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。

当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做：

首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 然后，从备份的时间点开始，将备份的binlog依次取出来，重放到中午误删表之前的那个时刻。

这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去

binlog会记录所有的逻辑操作，可以根据binlog来进行备份恢复。

但是考虑，执行update语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了crash。此过程如果不两阶段提交，就会出现如下问题：

先写binlog，再写redolog。 binlog写进去了然后崩溃了，但redolog没有完成。此时此事务失效，但按照binlog中的恢复，是已经完成事务的状态，这样就多了一个事务。

如果先写redolog，再写binlog。redolog写进去后崩溃了，Mysql进程异常重启，使用binlog中的恢复，则使已经完成的事务失效。

所以使用两阶段提交 ： redolog prepare -> binlog ->redolog commit

redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候， 表示每次事务的redo log都直接持久化到磁盘。这个参数我建议你设置成1，这样可以保证 MySQL异常重启之后数据不丢失。


sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数我也建 议你设置成1，这样可以保证MySQL异常重启之后binlog不丢失


##事务 ##

事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在Mysql中，事务支持是在引擎层实现的。

并不是所有引擎都支持事务，比如原生的MyISAM就不支持，InnoDB是支持的。

事务四大特性：ACID

Isolation: 隔离级别

当数据库上有多个事务同时执行的时候，可能出现脏读，不可重复读，幻读的问题。为了解决这些问题，就有了隔离级别的概念。

隔离性越强，效率越低，所以要在两者间寻求一个平衡点。

SQL的事务隔离级别包括：读未提交、读提交、可重复读、串行化 四个级别。

读未提交：一个事务未被提交，它做的变更就能被别的事务看到。

读提交：只一个事务提交后，其他事务才能看到它做的变更。

可重复读：一个事务执行过程中看到的数据，总是和事务启动时看到的数据是一致的。

串行化：对同一行记录，读写都会加锁。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。

![](../pics/s4.png)

读未提交： v1=2 v2=2 v3=2

读提交： v1=1 v2=2 v3=2

可重复读： v1=1 v2=1 v3=2

串行化： v1=1 v2=1 v3=2 事务B修改值的时候会被锁住，等待事务1提交才能继续。

在实现上，数据库会创建一个视图，访问的时候以事务的逻辑结构为准。

可重复读级别下： 这个视图是在事务启动时创建，整个事务期间都用这个视图。

读提交级别下：这个视图是在每个SQL语句开始执行时创建。

读未提交直接返回记录上的最新值，没有视图概念。

串行化直接采用加锁的方式来避免并行访问。

**事务隔离级别的实现**

在Mysql中，**实际上每条记录在更新的时候都会同时记录一条回滚操作**。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。

![](../pics/s5.png)

对于可重复读，每个事务都会创建一个视图。

不同时刻启动的事务会有不同的read-view

在视图A、B、C中，这一个记录的值分别为 1 ，2 ，4 同一条记录在系统中可能存在不同的版本，这就是数据库的多版本并发控制MVCC。

要得到1，就必须依次执行途中的所有回滚得到。
	
回滚日志在不需要的时候会被删除。系统会进行判断需不需要此回滚日志。

当系统中没有比这个回滚日志更早的read-view的时候---不需要

不推荐长事务的理由--意味着系统中存在很老的事务视图。

由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。

**事务的启动方式**

避免长事务：

1. 显示启动事务,begin或start transaction 配套提交commit 回滚 rollback
2. set autocommit=0 不会自动提交 执行一个select事务启动，并不会自动提交

有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的 查询都在事务中，如果是长连接，就导致了意外的长事务。

建议你总是使用set autocommit=1, 通过显式语句的方式来启动事务。

begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语 句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 

可以在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查 找持续时间超过60s的事务

	select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60




## 索引 ##

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。

**索引常见模型**

提高读写效率的数据结构 ：哈希表、有序数组和搜索树。

哈希索引做区间查询的速度是很慢的。如果相查给定区间的数据，只能全部扫描一遍。

哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎。

有序数组在等值查询和范围查询场景中的性能就都非常优秀。

假设身份证号没有重复，这个数组就是按照身份证号递增的顺序保存的。这时候如果你 要查ID_card_n2对应的名字，用二分法就可以快速得到，这个时间复杂度是O(log(N))

这个索引结构支持范围查询。

如果仅仅看查询效率，有序数组就是最好的数据结构了。但是，在需要更新数据的时候就麻烦了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高。

有序数组索引只适用于静态存储引擎。

二叉搜索树：

![](../pics/s6.png)

当然为了维持O(log(N))的查询复杂度，你就需要保持这棵树是平衡二叉树。为了做这个保证，更新的时间复杂度也是O(log(N))

多叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右 递增。二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因 是，索引不止存在内存中，还要写到磁盘上。

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小
                            
一棵100万节点的平衡二叉树，树高20。一次查询可能需要访问20个数据块。在 机械硬盘时代，从磁盘随机读一个数据块需要10 ms左右的寻址时间。也就是说，对于一个100 万行的表，如果使用二叉树来存储，单独访问一个行可能需要20个10 ms的时间，这个查询可真够慢的

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小


N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中。

MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。

**InnoDB的索引模型**

在InnoDB中，表是根据主键顺序以索引的形式存放的。InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。

每一个索引在InnoDB里面对应一棵B+树。

	mysql> create table T( id int primary key, k int not null, name varchar(16), index (k))engine=InnoDB;

![](../pics/s7.png)

索引类型分为主键索引和非主键索引。

主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）

非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 （secondary index）

**基于主键索引和普通索引查询有什么区别**

如果语句是select *fromTwhere ID=500，即主键查询方式，则只需要搜索ID这棵B+树；

如果语句是select *fromTwhere k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID 的值为500，再到ID索引树搜索一次。这个过程称为回表

基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。


**索引维护**

添加/删除 数据可以出现的 页分裂 和 页合并的情况。

自增主键： NOT NULL PRIMARY KEY AUTO_INCREMENT

插入数据可以不指定ID的值，系统会根据ID最大值+1作为下一条记录的ID的值。

自增主键的插入数据模式，是自增的插入模式，也就是每次插入一条新纪录，都是追加操作，都不涉及挪动其他记录，也不会出发叶子节点的分裂。

而将业务逻辑的字段作为主键，往往不能保证有序的插入，写数据成本较高。

还需要从存储的角度考虑。主键的长度越小，普通索引叶子节点越小，普通索引占用的空间也就越小。自增主键往往比较合理。

适合业务字段作为主键的场景 ： 1.只有一个索引 2.该索引为唯一索引

**回表**

	mysql> create table T ( 
	ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT '', index k(k)) engine=InnoDB; 

	insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'ff'),(700,7,'gg');

对于	select *fromTwhere k between 3 and 5

需要先在 k索引树上找到k=3的记录，取得ID=300。 再到ID索引树下查到ID=300对应的R3。 在k索引树下取到下一个值k=5,取得id.再到ID索引。。

回到主键索引树搜索的过程，称为回表。

**聚簇索引**

	* 如果表设置了主键，则主键就是聚簇索引
	* 如果表没有主键，则会默认第一个NOT NULL，且唯一（UNIQUE）的列作为聚簇索引
	* 以上都没有，则会默认创建一个隐藏的row_id作为聚簇索引

**普通索引**

	InnoDB的普通索引叶子节点存储的是主键（聚簇索引）的值，而MyISAM的普通索引存储的是记录指针。

**索引覆盖**

如 select ID from T where k between 3 and 5; 此时只需查ID的值，而ID已经在k索引树上了，不用回表。

**联合索引**

为了实现索引覆盖，即不用回表就可以直接在一棵索引树上查询结果。可以建立联合索引来优化sql。 即通过建立联合索引的方式，减少回表查询次数。

	create index idx_age_name on user(`age`,`name`);

![](../pics/s8.png)

**最左匹配原则**

索引项是按照索引定义里面出现的字段顺序排列的。

当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到ID4，然后向后遍历得到所有需要的结果。

如果你要查的是所有名字第一个字是“张”的人，你的SQL语句的条件是"where name like ‘张%’"。这时，你也能够用上这个索引，查找到第一个符合条件的记录是ID3，然后向后遍历， 直到不满足条件为止。

可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左 前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。

在建立联合索引时，如何安排索引内的字段顺序？

评估标准是索引的复用能力

第一原则是，组合索引如果通过调整顺序，可以少维护一个索引。那么这个顺序就是需要优先考虑使用的。

如果既有联合索引(a,b)，又要有基于a,b各自的查询，就需要考虑空间的问题。

如name字段比age字段大，所以可以维护(name,age)索引和(age)索引。

**索引下推**

最左前缀可以用于在索引中定位记录，而对于符合最左匹配但还有其余条件的部分：

如mysql> select * from tuser where name like '张%' and age=10 and ismale=1;

此语句在搜索索引树可以找到第一个满足条件的ID3

然后再判断其他条件是否满足：

在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。

而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

![](../pics/h1.png)

无索引下推将会直接回表查询。 将第一个字为“张”的一个一个拿去回表查询，共回表四次。

![](../pics/h2.png)

InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的 记录直接判断并跳过。此例子中只需回表两次。


**数据库锁**

全局锁 表锁  行锁

**全局锁**

加锁方式： FLUSH tables with read lock

当需要整个库处于只读状态，可以使用此命令，之后的其他线程会被以下语句所阻塞

数据更新语句、数据定义语句、更新事务的提交语句

全局锁的应用场景：全局逻辑备份。就是把每个表都select出来存成文本

不加锁的话，备份系统备份得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。--可以通过可重复读隔离级别得到一致性视图。

即在可重复读隔离级别下开启下一事务。 

对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。

如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。

**表级锁**

Mysql中的表级锁有两种，一种是表锁、一种是元数据锁。

加锁方式：lock tables .. read/write 也可以通过unlock主动释放锁，也可以在客户端断开的时候自动释放。

lock tables语法除了会限制别的线程的读写 外，也限定了本线程接下来的操作对象

如果在某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读 写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操 作。连写t1都不允许，自然也不能访问其他表。

在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于InnoDB这种支持 行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。

另一类表级锁是meta lock。 元数据锁

MDL不需要显式使用，在访问一个表的时候会被 自动加上。MDL的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个 表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果 跟表结构对不上，肯定是不行的

在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。

读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。

读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线 程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

![](../pics/l1.png)

sessionA sessionB 都给表t加了读锁，此时sessionC无法申请MDL写锁，所以被阻塞了，影响了接下来对t申请读锁的操作。

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session 再请求的话，这个库的线程很快就会爆满

事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。

如何安全地给小表加字段？

要解决长事务，事务不提交，就会一直占着MDL锁， kill掉长事务

在alter table语句里面 设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后 面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。


**Mysql行锁**

Mysql的行锁是在引擎级别由各个引擎自己实现的。但并不是每个引擎都支持行锁。如MyISAM就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同 一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。

InnoDB的行锁：

行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。

![](../pics/l2.png)

事务A持有的两个记录的行锁，都是在commit的时候才释放的。

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。

--如果你的事务需要锁住多行，要把最可能造成锁冲突，最可能影响并发度的锁尽量后放。

**死锁和死锁检测**

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。


![](../pics/l3.png)

事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和 事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略：

一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout来设置.

在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

同时超时时间设置太短的话，会出现很多误伤


另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。


但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤

主动死锁检测，而且 innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发 现并进行处理的，但是它也是有额外负担的。

每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级 的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到 CPU利用率很高，但是每秒却执行不了几个事务。

怎么解决由这种热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的CPU资源

一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严 重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关 掉死锁检测意味着可能会出现大量的超时，这是业务有损的。

另一种方法是控制并发度。，你会发现如果并发能够控制住，比如同一行同时 最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。

这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新， 在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了

可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如10个记录，影院的账户总额等于这10个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的1/10，可以减少锁等待个数，也就减少了死锁检测的CPU消耗


**事务隔离**

可重复读隔离级别：事务T启动的时候会创建一个视图read-view，之后事务T执行期间，即使有其他的事务修改了数据，事务T看到的仍然和启动时看到的是一样的。

行锁：一个事务要更新一行，如果另外事务拥有这一行的行锁，会被锁住，进入等待状态。那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？


在Mysql中，有两个关于视图的概念：

一个是view ， 查询语句定义的虚拟表

一个是consistent read view，用于支持读提交和可重复读隔离级别的实现。

在可重复读级别，事务在启动时就拍了个快照。这个快照是基于整库的。

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向 InnoDB的事务系统申请的，是按申请顺序严格递增的。

![](../pics/t2.png)

数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的rowtrx_id。

每次事务更新数据的时候，都会生成一个新的数据版本，并且 把transaction id赋值给这个数据版本的事务ID，记为rowtrx_id。

同时，旧的数据版本要保留， 并且在新的数据版本中，能够有信息可以直接拿到它。

语句更新会生成undo log（回滚日志）

实际上，图中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而 是每次需要的时候根据当前版本和undo log计算出来的。

比如，需要V2的时候，就是通过V4依 次执行U3、U2算出来

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这 个事务执行期间，其他事务的更新对它不可见。

因此，一个事务只需要在启动的时候声明说，“以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成的，我就不认，我必须要找到它的上一个版本”。

当然，如果“上一个版本”也不可见，那就得继续往前找。还有，如果是这个事务自己更新的数据，它自己还是要认的

在实现上， InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活 跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。

数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。

这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

数据版本的可见性规则，就是基于数据的rowtrx_id和这个一致性视图的对比结果得到的。

![](../pics/t3.png)

这样，对于当前事务的启动瞬间来说，一个数据版本的rowtrx_id，有以下几种可能：

1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是 可见的；

2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；

3. 如果落在黄色部分，那就包括两种情况 a. 若 rowtrx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 rowtrx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。


![](../pics/t1.png)


![](../pics/t4.png)

1. 事务A开始前，系统里面只有一个活跃事务ID是99；
2. 事务A、B、C的版本号分别是100、101、102，且当前系统里只有这四个事务； 
3. 三个事务开始前，(1,1）这一行数据的rowtrx_id是90。

事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101], 事务C的视图数组是 [99,100,101,102].

第一个有效更新是事务C，把数据从(1,1)改成了(1,2)。这时候，这个数据的最 新版本的rowtrx_id是102，而90这个版本已经成为了历史版本。

第二个有效更新是事务B，把数据从(1,2)改成了(1,3)。这时候，这个数据的最新版本（即row trx_id）是101，而102又成为了历史版本

在事务A查询的时候，其实事务B还没有提交，但是它生成的(1,3)这个版本已 经变成当前版本了。但这个版本对事务A必须是不可见的，否则就变成脏读了.

现在事务A要来读数据了，它的视图数组是[99,100]。当然了，读数据都是从当前版本读起 的。所以，事务A查询语句的读数据流程是这样的

找到(1,3)的时候，判断出rowtrx_id=101，比高水位大，处于红色区域，不可见；

接着，找到上一个历史版本，一看rowtrx_id=102，比高水位大，处于红色区域，不可见；

再往前找，终于找到了（1,1)，它的rowtrx_id=90，比低水位小，处于绿色区域，可见。

这样执行下来，虽然期间这一行数据被修改过，但是事务A不论在什么时候查询，看到这行数据 的结果都是一致的，所以我们称之为一致性读。

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况:

1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。

对于update语句，都是先读再写的。而这个读，只能读当前值。

![](../pics/t5.png)

Select语句加锁的话，也是采用的当前读。

	mysql> select k from t where id=1 lock in share mode; 
	mysql> select k from t where id=1 for update;


![](../pics/t6.png)

事务C在更新后没有立刻提交，在它提交前，事务B的更新语句已经发起了。

虽然C还未提交，但已生成了一个最新版本(1,2)，事务C未提交，(1,2)的写锁未释放。

而事务B是当前读，必须读最新版本，而且必须加锁，所以就进入了等待状态。


事务的可重复读如何实现？

两阶段锁协议：在InnoDB I 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。

可重复读的核心就是一致性读，而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：

在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询 都共用这个一致性视图；

在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。



**唯一索引和普通索引**


普通索引可以使用change buffer优化机制

InnoDB是按数据页为单位读写的。当需要一条记录时，并不是将这个记录从磁盘中读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小是16k.


对于普通索引，查找k=5的记录后，需要查找下一个记录，直到找到k!=5的

对于唯一索引，查找k=5的记录后直接返回就行了。

查询操作，普通索引和唯一索引相差不大。

而对于更新操作。普通索引可以借助change buffer 机制，但唯一索引不行。

change Buffer : 

当需要更新一个数据页时，如果这个数据页已经在内存中了就直接更新，而如果这个数据页没有在内存中的话，在不影响一致性的前提下。InnoDB会将这些更新操作缓存在change buffer中。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页相关的操作。

Change Buffer实际上也是持久化的数据，也会从内存中拷到硬盘里。

将ChangeBuffer中的操作应用到原数据页，此过程成为merge。除了访问这个数据会触发merge，系统也会有后台线程定期merge。在数据库正常关闭时也会merge。

对于唯一索引来说，每个更新操作都需要判断此操作是否违反唯一性的约束。而这必须将数据页读入内存才能判断。如果都读到内存了，直接写入更快。

因此唯一索引更新不能使用change buffer。实际上只有普通索引能用。

将数据从磁盘读入内存设计随机IO的访问，时数据库里成本最高的操作，changebuffer减少了随机磁盘访问，提升了性能。

ChangeBuffer的使用场景：

merge的时候是真正进行更新的时候，changeBuffer的作用就是将记录的变更动作缓存起来，所以在一个数据页merge时，changeBuffer记录的变更越多，也就是之前更新操作越多，收益越大。

对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。

如果所有更新后面都伴随着对此记录的查询，则应关闭changeBuffer。

普通索引和change buffer的配合使用，对于数据量大的表的更新优 化还是很明显的


	insert into t(id,k) values(id1,k1),(id2,k2);

假设当前k索引树的状态，查找到位置后，k1所在的数据页在内存(InnoDB buffer pool)中，k2所在的数据页不在内存中


![](../pics/t7.png)

这条语句做了如下操作：

1. Page1在内存空间中，直接更新内存写入磁盘
2. Page2在磁盘中，直接将写入操作写入change buffer
3. 将上述两个操作写入redo log

此操作只写了两处内存和一次磁盘。

在此之后的读操作 select *fromt where k in (k1, k2)

如果读操作发生在更新操作不久之后，内存中的数据都还在，那么此时的这两个读操作就与系统表 空间（ibdata1）和 redo log无关了

![](../pics/t8.png)


读Page 1的时候，直接从内存返回。要读Page 2的时候，需要把Page 2从磁盘读入内存中，然后应用change buffer里面的操作 日志，生成一个正确的版本并返回结果。

redo log 主要节省的是随主机写磁盘的机IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗。
 

在一些“归档库”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半年，然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高 归档效率，可以考虑把表里面的唯一索引改成普通索引。







                                                
